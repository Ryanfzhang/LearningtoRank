#!/usr/bin/env python3
# encoding: utf-8
# coding style: pep8
# ====================================================
#   Copyright (C)2019 All rights reserved.
#
#   Author        : Eskimo
#   Email         : zhangfaninner@163.com
#   File Name     : attBasedRnn.py
#   Last Modified : 2019-09-16 16:40
#   Describe      :
#
# ====================================================

import sys
# import os
import os
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torch import optim

from org.l2r_global import L2R_GLOBAL
device = L2R_GLOBAL.global_device


class MultiHeadAttention(nn.Module):
    def __init__(self, n_head, d_input, d_k, d_v, dropout = 0.1):
        super().__init__()
        #params
        self.n_head = n_head
        self.d_k = d_k
        self.d_v = d_v
        self.d_input = d_input
        self.dropout = dropout

        self.w_q = nn.Linear(d_input, n_head*d_k)
        self.w_k = nn.Linear(d_input, n_head*d_k)
        self.w_v = nn.Linear(d_input, n_head*d_v)
        self.layer_norm = nn.LayerNorm(d_input)
        self.fc = nn.Linear(n_head*d_v, d_input)
        self.Dropout = nn.Dropout(self.dropout)

        self.weight_init()
    def weight_init(self):
        nn.init.normal_(self.w_q.weight, mean=0, std=np.sqrt(2.0/ (self.d_input + self.d_k)))
        nn.init.normal_(self.w_k.weight, mean=0, std=np.sqrt(2.0/ (self.d_input + self.d_k)))
        nn.init.normal_(self.w_v.weight, mean=0, std=np.sqrt(2.0/ (self.d_input + self.d_k)))
        nn.init.xavier_normal_(self.fc.weight)
    
    def forward(self, q, k, v):
        bs, seq_len_q, _ = q.shape
        bs, seq_len_k, _ = k.shape
        bs, seq_len_v, _ = v.shape

        residual = q
        q = self.w_q(q).view(bs, seq_len_q, self.n_head, self.d_k)
        k = self.w_k(k).view(bs, seq_len_k, self.n_head, self.d_k)
        v = self.w_v(v).view(bs, seq_len_v, self.n_head, self.d_v)

        q = q.permute(2, 0, 1, 3).contiguous().view(-1, seq_len_q, self.d_k)
        k = k.permute(2, 0, 1, 3).contiguous().view(-1, seq_len_k, self.d_k)
        v = v.permute(2, 0, 1, 3).contiguous().view(-1, seq_len_v, self.d_v)

        attention = torch.matmul(q, k.transpose(1,2))#nb, seq, seq
        attention = attention/np.power(self.d_k,0.5)
        attention = F.softmax(attention, dim=2)
        output = torch.matmul(attention, v)
        
        output = output.view(self.n_head, bs, seq_len_q, self.d_v)
        output = output.permute(1, 2, 0, 3).contiguous().view(bs, seq_len_q, -1)

        output = self.Dropout(self.fc(output))
        output = self.layer_norm(residual+ output)

        return output
 
class PositionwiseFeedForward(nn.Module):
    ''' A two-feed-forward-layer module '''

    def __init__(self, d_in, d_hid, dropout=0.1):
        super().__init__()
        self.w_1 = nn.Conv1d(d_in, d_hid, 1) # position-wise
        self.w_2 = nn.Conv1d(d_hid, d_in, 1) # position-wise
        self.layer_norm = nn.LayerNorm(d_in)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        residual = x
        output = x.transpose(1, 2)
        output = self.w_2(F.relu(self.w_1(output)))
        output = output.transpose(1, 2)
        output = self.dropout(output)
        output = self.layer_norm(output + residual)
        return output

class Encoder(nn.Module):   
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.multiAttention = MultiHeadAttention(1, args.hidden_size, args.hidden_size, args.hidden_size)
        self.positionForward = PositionwiseFeedForward(args.hidden_size, args.hidden_size)
    def forward(self, x):
        output = self.multiAttention(x , x, x)
        output = self.positionForward(output)
        return output

class Attention(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args        
        self.input_linear = nn.Linear(args.hidden_size, args.hidden_size)
        self.context_linear = nn.Conv1d(args.hidden_size, args.hidden_size, 1, 1)
        self.V = nn.Parameter(torch.FloatTensor(args.hidden_size), requires_grad=True)
        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)

        self.tanh= nn.Tanh()
        self.softmax = nn.Softmax()
        nn.init.uniform_(self.V, -1, 1)

    def forward(self, input, context, mask):

        inp = self.input_linear(input).unsqueeze(2).expand(-1, -1, context.size(1)) #(b, e, s)
        context = context.permute(0, 2, 1)
        ctx = self.context_linear(context)#(b, e, s)

        V= self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)#(b, 1, e)
        att = torch.bmm(V, self.tanh(inp+ctx)).squeeze(1)

        if len(att[mask]) > 0:
            att[mask] = self.inf[mask]
        alpha = self.softmax(att)

        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)

        return hidden_state, alpha
    def init_inf(self, mask_size):
        self.inf = self._inf.unsqueeze(1).expand(*mask_size)

class Decoder(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args =args
        self.gru = nn.GRUCell(args.hidden_size, args.hidden_size)
        self.att = Attention(args)
        self.mask = nn.Parameter(torch.ones(1), requires_grad = False)
        self.hidden_out  = nn.Linear(2* args.hidden_size, args.hidden_size)

    def forward(self, embedded_inputs, decoder_input_0, hidden_0, context, label):
        bs, seq_len, _ = embedded_inputs.size()

        mask = self.mask.repeat(seq_len).unsqueeze(0).repeat(batch_size, 1).bool()
        self.att.init_inf(mask.size())
        
        runner = torch.arange(seq_len).unsqueeze(0).expand(bs, -1).to(device)
        permuted_doc_label, pros_list, reward_list, indices=[], [], [], []
        doc = decoder_input_0
        hidden = hidden_0

        for _ in range(seq_len):
            h_t = self.gru(doc, hidden)
            h_t_p , outs = self.att(h_t, context, mask)
            hidden = torch.tanh(self.hidden_out(torch.cat((h_t_p, h_t),1)))
            
            pro, indice = out.max(1)
            
            one_hot_pointer = (runner== indice.unsqueeze(1).expand(-1, outs.size(1)))
            mask = mask*(~one_hot_pointer)
            embedding_mask = one_hot_pointer.unsqueeze(2).expand(-1, -1, self.args.hidden_size)
            doc = torch.masked_select(embedded_inputs, embedding_mask).view(bs,-1)
            permuted_doc_label.append(torch.masked_select(label, one_hot_pointer).view(bs))
            pros_list.append(pro)
            reward_list.append(self.reward_ndcg(permuted_doc_label))
            indices.append(indice)
        assert torch.sum(mask)==0
        return reward_list, pros_list, permuted_doc_label, indices
    def reward_ndcg(self, label_list):
        return (2**label_list[-1]-1.)/np.log2(len(label_list)+1.)

class attBasedRnn(nn.Module):

    def __init__(self, args):
        super().__init__()
        self.args = args
        self.fnn = nn.Sequential(
            nn.Linear(args.feature_size, args.hidden_size),
            nn.ReLU(),
            nn.Dropout(args.dropout),
            nn.Linear(args.hidden_size, args.hidden_size),
            nn.Sigmoid(),
            )
        self.Encoder = Encoder(args)
        self.Decoder = Decoder(args)
        self.h = nn.Parameter(torch.zeros(args.hidden_size),requires_grad=False)

    def forward(self, batch_ranks, label):
        bs, seq_len, _ = batch_ranks.size()
        batch_doc_reprs = self.fnn(batch_ranks)
        outs = self.Encoder(batch_doc_reprs)
        embedded_inputs = batch_doc_reprs.detach()
        cnt_feature = torch.max(batch_doc_reprs,dim =1)[0]
        reward_list, pros_list, permuted_doc_label, indices = self.Decoder(embedded_inputs, cnt_feature, self.h.unsqueeze(0).expand(bs, -1), outs , label)

        probs = torch.cat(pros_list, dim =1)
        rewards = torch.cat(reward_list,dim =1)
        rewards = rewards*(self.args.discount ** torch.arange(seq_len).unsqueeze(0).expand(bs, -1).float().to(device))
        R = torch.cumsum( rewards.flip((1)), dim =1).flip((1))
        self.loss = -torch.sum( R*probs)
        self.indices = torch.cat(indices, dim =1)
        
        return 0.9**(torch.sort(self.indices, dim = -1, descending=False)[1].float())

    def get_parameters(self):
        return list(self.parameters())

class attRNN(object):
    def __init__(self, args, id= None, ranking_function = None, opt="Adam", lr = 1e-3, weight_decay =1e-3):
        self.ranking_function = rf(args)
        self.opt = opt
        self.lr = lr
        self.weight_decay = weight_decay
        self.init_optimizer()

    def reset_parameters(self):
        pass

    def init_optimizer(self):
        if 'Adam' == self.opt:
            self.optimizer = optim.Adam(self.ranking_function.get_parameters(), lr=self.lr, weight_decay=self.weight_decay)  # use regularization
        elif 'RMS' == self.opt:
            self.optimizer = optim.RMSprop(self.ranking_function.get_parameters(), lr=self.lr, weight_decay=self.weight_decay)  # use regularization
        else:
            raise NotImplementedError
    def train(self, batch_ranks, batch_std , train=True):
        self.ranking_function.train()
        ranking = self.ranking_function(batch_ranks, batch_std)

        self.optimizer.zero_grad()
        self.ranking_function.loss.backward()
        self.optimizer.step()
        return self.ranking_function.loss

    def predict(self, batch_ranks, batch_std, train=False):
        self.ranking_function.eval()
        with torch.no_grad():
            ranking = self.ranking_function(batch_ranks, batch_std)
        return ranking 

    def save_model(self, dir, name):
        if not os.path.exists(dir):
            os.makedirs(dir)
        torch.save(self.ranking_function.state_dict(), dir+'/'+name+'.torch')

    def load(self, name):
        self.ranking_function= torch.load(name)
